
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>MICCAI 2024 Challenge on Long-tailed, multi-label, and zero-shot classification on chest X-rays</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body>

<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="800" align="center" valign="middle"><h3>MICCAI 2024 Challenge</h3>
      <span class="title">CXR-LT: Long-tailed, multi-label, and zero-shot classification on chest X-rays</span></td>
    </tr>
    <tr>
    <h2><td colspan="3" align="center"><br> 
      Location: Marrakesh, Morocco<br>
      Time: <b>TBD, October 2024</b>
    </td>
    </h2>
    </tr>
  </table>
<!--   <p><img src="figures/teaser.jpg" width="1000" align="middle" /></p> -->
</div>

<br>

<div class="containersmall">
  <center><b>Click here to participate in Task 1: <a target="_blank" rel="noopener noreferrer" href="https://codalab.lisn.upsaclay.fr/competitions/18601">https://codalab.lisn.upsaclay.fr/competitions/18601</a></b></center>
  <center><b>Click here to participate in Task 2: <a target="_blank" rel="noopener noreferrer" href="https://codalab.lisn.upsaclay.fr/competitions/18603">https://codalab.lisn.upsaclay.fr/competitions/18603</a></b></center>
  <center><b>Click here to participate in Task 3: <a target="_blank" rel="noopener noreferrer" href="https://codalab.lisn.upsaclay.fr/competitions/18604">https://codalab.lisn.upsaclay.fr/competitions/18604</a></b></center>
</div>

<br>


<div class="container">
  <h2>Overview</h2>
  <div class="overview">
    <p>Chest radiography, like many diagnostic medical exams, produces a <strong>long-tailed</strong> distribution of clinical findings; while a small subset of diseases is routinely observed, the vast majority of diseases are relatively rare [1]. This poses a challenge for standard deep learning methods, which exhibit bias toward the most common classes at the expense of the important but rare "tail" classes [2]. Many existing methods [3] have been proposed to tackle this specific type of imbalance, though only recently has attention been given to long-tailed medical image recognition problems [4-6]. Diagnosis on chest X-rays (CXRs) is also a <strong>multi-label</strong> problem, as patients often present with multiple disease findings simultaneously; however, only a select few studies incorporate knowledge of label co-occurrence into the learning process [7-9, 12]. Since most large-scale image classification benchmarks contain single-label images with a mostly balanced distribution of labels, many standard deep learning methods fail to accommodate the class imbalance and co-occurrence problems posed by the long-tailed, multi-label nature of tasks like disease diagnosis on CXRs [2].</p>

    <p>In the first iteration of CXR-LT held in 2023, we expanded upon the <a href="https://physionet.org/content/mimic-cxr-jpg/2.0.0/">MIMIC-CXR-JPG</a> [10,11] dataset by enlarging the set of target classes from 14 to 26, generating labels for 12 new rare disease findings by parsing radiology reports [13]. While this made for a challenging long-tailed, multi-label disease classification task that attracted 59 teams who contributed over 500 unique submissions, Radiology Gamuts Ontology documents over 4,500 unique radiological image findings. That is, the "true" distribution of all clinical findings on CXR is at least two orders of magnitude longer than what our -- or any existing dataset can offer. For this reason, we argue that the only way to truly tackle the long-tail of radiological image findings is to develop a model that can readily generalize to new classes in <strong>"zero-shot"</strong> fashion [14].</span></span></p>

    <p><span><span>For this year's version of CXR-LT, we extract labels for an additional 19 rare disease findings (for a total of 377,110 CXR images, each with 45 disease labels) and introduce two new challenge tracks, featuring a zero-shot classification task.</p>  </div>
</div>

<br>

<div class="container">
  <h2>Shared Task</h2>
    <div class="overview">
      <h3> Dataset </h3>
      <p>In the first iteration of CXR-LT held in 2023, we expanded upon the MIMIC-CXR dataset by enlarging the set of target classes from 14 to 26, generating labels for 12 new rare disease findings by parsing radiology reports. For this year's version of CXR-LT, we extract labels for an additional 19 rare disease findings (for a total of 377,110 CXR images, each with 45 disease labels).</p>

      <h3>Task</h3>
      <p>Given a CXR, our challenge includes three tasks, to be held as independent tasks: 
        <ul>
          <li>Task 1: long-tailed classification on a large, noisy test set;</li>
          <li>Task 2: long-tailed classification on a small, manually annotated test set;</li>
          <li>Task 3: zero-shot generalization to previously unseen disease findings. </li>
        </ul>
      For all tasks, participants will be provided with a large, automatically labeled training set of >250,000 CXR images with 40 binary disease labels. While last year's CXR-LT was a success, we hope that CXR-LT 2024 can provide even further meaningful methodological advances toward clinically realistic multi-label, long-tailed, and zero-shot disease classification on CXR.
      </p>

      <h3>Evaluation</h3>
      <p>For each task, models will be evaluated on the provided testing set using “macro-averaged” mean Average Precision (<strong>mAP</strong>). 
        <ul>
          <li>Task 1 will be evaluated on a large, automatically labeled test set of >75,000 CXRs from these same 40 labels;</li>
          <li>Task 2 will be evaluated on a "gold standard" subset of the test set, containing 409 CXRs from 26 of the 40 labels that were manually annotated;</li>
          <li>Task 3 will be evaluated on the same large test set of images as task (1), but for 5 "held-out" disease findings that have not been encountered during training.</li>
        </ul>
        </p>
      </p>

      <h3>Online Evaluation</h3>
      <p>The competition will be conducted through the <a href="https://codalab.lisn.upsaclay.fr/">CodaLab</a> platform.
      </p>
    </div>
</div>

<br>

<div class="container">
  <h2>To Participate</h2>
    <div class="overview">
      <p>
      This competition uses data from <a href="https://physionet.org/content/mimic-cxr-jpg/2.0.0/">MIMIC-CXR-JPG v2.0.0</a>, which requires credentialing through <a href="https://physionet.org/">PhysioNet</a> and a signed Data Use Agreement (DUA) for MIMIC-CXR-JPG. To participate in this competition, you must:
      <li>Become a credentialed user through <a href="https://physionet.org/">PhysioNet</a> and sign the DUA for MIMIC-CXR-JPG v2.0.0 access. (If you are already credentialed with MIMIC-CXR-JPG access, you can skip this step.)</li>>
      <li>Fill out this <a href="https://forms.gle/k8bApLNy85rgnTRc6">Google Form</a> providing (i) your <strong>CodaLab email address</strong>, (ii) <strong>proof that you are a credentialed PhysioNet user</strong>, and (iii) <strong>proof that you signed the DUA for MIMIC-CXR-JPG v2.0.0</strong>.</li>
      <li>Register for the competition on CodaLab via the "Participate" tab and await our review. </li>
      If you have completed these steps correctly, you will be admitted to the competition and we will provide links to download the necessary data by email! You are not permitted to share these labels whatsoever.
      </p>
    </div>
</div>

<br>

<div class="container">
  <h2>Tentative Schedule</h2>
    <div class="schedule">
      <p class="subtitle"><span class="announce_date">May 01, 2024</span>. Training data released and challenge (development phase) begins</p>
      <p class="subtitle"><span class="announce_date">Aug 01, 2024</span>. Test labels released and final evaluation (testing phase) begins</p>
      <p class="subtitle"><span class="announce_date">Aug 04, 2024</span>. Testing phase ends and competition is closed.</p>
      <p class="subtitle"><span class="announce_date">Aug 15, 2024</span>. Top-performing teams invited to present at MICCAI 2024</p>
      <p class="subtitle"><span class="announce_date">Oct 10, 2024</span>. MICCAI 2024 CXR-LT Challenge event</p>
    </div>
</div>

<br>

<div class="containersmall">
  <h2>Steering committee</h2>
    <div class="instructor">
    <a href="https://vivo.weill.cornell.edu/display/cwid-ges9006">
      <div class="instructorphoto"><img src="figures/george_shih.jpg"></div>
      <div>George Shih<br>Weill Cornell Medicine<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://www.ncbi.nlm.nih.gov/research/bionlp/Zhiyong-Lu">
      <div class="instructorphoto"><img src="figures/zhiyong_lu.png"></div>
      <div>Zhiyong Lu<br>NIH/NLM/NCBI<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://irp.nih.gov/pi/ronald-summers">
      <div class="instructorphoto"><img src="figures/rsummers.jpg"></div>
      <div>Ronald M. Summers<br>NIH Clinical Center<br></div>
    </a>
  </div>

      <div class="instructor">
    <a href="https://www.hsph.harvard.edu/ecpe/faculty/leo-anthony-celi/">
      <div class="instructorphoto"><img src="figures/Leo.png"></div>
      <div>Leo Anthony Celi<br>MIT/Harvard<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://www.jefferson.edu/academics/colleges-schools-institutes/skmc/departments/radiology/faculty-staff/faculty/flanders.html">
      <div class="instructorphoto"><img src="figures/adam_flanders.jpg"></div>
      <div>Adam E. Flanders<br>Thomas Jefferson University<br></div>
    </a>
  </div>


</div>

<br>

<div class="containersmall">
  <h2>Organizers</h2>
  
  <div class="instructor">
    <a href="https://med.umn.edu/bio/mingquan-lin">
      <div class="instructorphoto"><img src="figures/MINGQUAN_LIN.jpg"></div>
      <div>Mingquan Lin<br>University of Minnesota<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://gholste.me/">
      <div class="instructorphoto"><img src="figures/greg_holste.jpg"></div>
      <div>Greg Holste<br>UT Austin<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://wsong1106.github.io/">
      <div class="instructorphoto"><img src="figures/song_wang.jpg"></div>
      <div>Song Wang<br>UT Austin<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://www.linkedin.com/in/yiliangzhou114">
      <div class="instructorphoto"><img src="figures/yiliang_zhou.jpeg"></div>
      <div>Yiliang Zhou<br>Weill Cornell Medicine<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/jhc">
      <div class="instructorphoto"><img src="figures/hao_chen.jpg"></div>
      <div>Hao Chen<br>HKUST<br></div>
    </a>
  </div>

  <div class="instructor">
    <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang#:~:text=Professor%20Zhangyang%20%E2%80%9CAtlas%E2%80%9D%20Wang%20is,University%20of%20Texas%20at%20Austin.">
      <div class="instructorphoto"><img src="figures/atlas_wang.jpg"></div>
      <div>Atlas Wang<br>UT Austin<br></div>
    </a>
  </div>

    <div class="instructor">
    <a href="https://penglab.weill.cornell.edu/">
      <div class="instructorphoto"><img src="figures/yifan_peng.jpg"></div>
      <div>Yifan Peng<br>Weill Cornell Medicine<br></div>
    </a>
  </div>
</div>

<br>

<div class="container">
  <h2>References</h2>
    <div class="overview">
      <ol>
        <li>Zhou SK, Greenspan H, Davatzikos C, Duncan JS, Van Ginneken B, Madabhushi A, Prince JL, Rueckert D, Summers RM. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises. Proceedings of the IEEE. 2021 Feb 26;109(5):820-38.</li>
        <li>Holste G, Wang S, Jiang Z, Shen TC, Shih G, Summers RM, Peng Y, Wang Z. Long-Tailed Classification of Thorax Diseases on Chest X-Ray: A New Benchmark Study. In Data Augmentation, Labelling, and Imperfections: Second MICCAI Workshop, DALI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings 2022 Sep 16 (pp. 22-32). Cham: Springer Nature Switzerland.</li>
        <li>Zhang Y, Kang B, Hooi B, Yan S, Feng J. Deep long-tailed learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023 Apr 19.</li>
        <li>Zhang R, Haihong E, Yuan L, He J, Zhang H, Zhang S, Wang Y, Song M, Wang L. MBNM: multi-branch network based on memory features for long-tailed medical image recognition. Computer Methods and Programs in Biomedicine. 2021 Nov 1;212:106448.</li>
        <li>Ju L, Wang X, Wang L, Liu T, Zhao X, Drummond T, Mahapatra D, Ge Z. Relational subsets knowledge distillation for long-tailed retinal diseases recognition. In Medical Image Computing and Computer Assisted Intervention&ndash;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&ndash;October 1, 2021, Proceedings, Part VIII 24 2021 (pp. 3-12). Springer International Publishing.</li>
        <li>Yang Z, Pan J, Yang Y, Shi X, Zhou HY, Zhang Z, Bian C. ProCo: Prototype-Aware Contrastive Learning for Long-Tailed Medical Image Classification. In Medical Image Computing and Computer Assisted Intervention&ndash;MICCAI 2022: 25th International Conference, Singapore, September 18-22, 2022, Proceedings, Part VIII 2022 Sep 16 (pp. 173-182). Cham: Springer Nature Switzerland.</li>
        <li>Chen H, Miao S, Xu D, Hager GD, Harrison AP. Deep hierarchical multi-label classification of chest X-ray images. In International Conference on Medical Imaging with Deep Learning 2019 May 24 (pp. 109-120). PMLR.</li>
        <li>Wang G, Wang P, Cong J, Liu K, Wei B. BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label Chest X-Ray Recognition. arXiv preprint arXiv:2302.11082. 2023 Feb 22.</li>
        <li>Chen B, Li J, Lu G, Yu H, Zhang D. Label co-occurrence learning with graph convolutional networks for multi-label chest x-ray image classification. IEEE Journal of Biomedical and Health Informatics. 2020 Jan 16;24(8):2292-302.</li>
        <li>Johnson AE, Pollard TJ, Greenbaum NR, Lungren MP, Deng CY, Peng Y, Lu Z, Mark RG, Berkowitz SJ, Horng S. MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. 2019 Jan 21.</li>
        <li> PhysioNet. MIMIC-CXR-JPG - chest radiographs with structured labels [Internet]. Available from: https://physionet.org/content/mimic-cxr-jpg/2.0.0/.</li>
        <li>Moukheiber D, Mahindre S, Moukheiber L, Moukheiber M, Wang S, Ma C, Shih G, Peng Y, Gao M. Few-Shot Learning Geometric Ensemble for Multi-label Classification of Chest X-Rays. In Data Augmentation, Labelling, and Imperfections: Second MICCAI Workshop, DALI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings 2022 Sep 16 (pp. 112-122). Cham: Springer Nature Switzerland.</li>
        <li> CodaLab. CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays [Internet]. Available from: https://codalab.lisn.upsaclay.fr/competitions/12599.</li>
        <li> Holste G, Zhou Y, Wang S, Jaiswal A, Lin M, Zhuge S, Yang Y, Kim D, Nguyen-Mau TH, Tran MT, Jeong J. Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge. arXiv preprint arXiv:2310.16112. 2023 Oct 24.</li>
      </ol>    </div>
</div>

<br>

<div class="containersmall">
    <p>Please contact <a href="mailto:cxrltchallenge2024@gmail.com">cxrltchallenge2024@gmail.com</a> if you have any questions. This webpage template is by courtesy of <a href="https://gkioxari.github.io/">Georgia</a>.</p>
    <!-- <p>This comeptition is sponsored in part by the Artifical Intelligence Journal (AIJ).</p> -->
    <!-- <center><img src="https://aij.ijcai.org/wp-content/uploads/2021/07/ARTINT_Logo2_c_web_more.jpg" width="10%"></img></center> -->
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>

